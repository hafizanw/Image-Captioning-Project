{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99bbfec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "IMAGE CAPTIONING - FLICKR8K (CNN-LSTM)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INSTALLATION LIBRARIES\n",
    "# ============================================================================\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Embedding, LSTM, Dropout, Add\n",
    ")\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"IMAGE CAPTIONING - FLICKR8K (CNN-LSTM)\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af66d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PATH CONFIG\n",
    "# ============================================================================\n",
    "DATA_DIR = \"../data\"\n",
    "FEATURE_DIR = os.path.join(DATA_DIR, \"features\")\n",
    "CHECKPOINT_DIR = \"../checkpoints\"\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f483f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total image features: 8091\n",
      "Total images with captions: 8091\n",
      "Vocab size: 2567\n",
      "Max caption length: 32\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "# Image Features\n",
    "with open(os.path.join(FEATURE_DIR, \"image_features.pkl\"), \"rb\") as f:\n",
    "    image_features = pickle.load(f)\n",
    "\n",
    "# Caption Features\n",
    "with open(os.path.join(FEATURE_DIR, \"image_to_captions.pkl\"), \"rb\") as f:\n",
    "    image_to_captions = pickle.load(f)\n",
    "\n",
    "# Vocabulary Mapping\n",
    "with open(os.path.join(FEATURE_DIR, \"word_to_idx.pkl\"), \"rb\") as f:\n",
    "    word_to_idx = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(FEATURE_DIR, \"idx_to_word.pkl\"), \"rb\") as f:\n",
    "    idx_to_word = pickle.load(f)\n",
    "vocab_size = len(word_to_idx) + 1\n",
    "\n",
    "# Max Caption Length\n",
    "all_captions = []\n",
    "for caps in image_to_captions.values():\n",
    "    all_captions.extend(caps)\n",
    "\n",
    "max_length = max(len(c.split()) for c in all_captions)\n",
    "\n",
    "\n",
    "print(\"Total image features:\", len(image_features))\n",
    "print(\"Total images with captions:\", len(image_to_captions))\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Max caption length:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa3aad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image ID: 1000268201_693b08cb0e.jpg\n",
      "Feature shape: (2048,)\n",
      "Sample captions:\n",
      "- startseq seorang anak dengan gaun merah muda sedang menaiki seperangkat tangga dengan jalan masuk endseq\n",
      "- startseq seorang gadis pergi ke sebuah bangunan kayu endseq\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SANITY CHECK: PRINT A SAMPLE\n",
    "# ============================================================================\n",
    "sample_img_id = list(image_to_captions.keys())[0]\n",
    "\n",
    "print(\"Image ID:\", sample_img_id)\n",
    "print(\"Feature shape:\", image_features[sample_img_id].shape)\n",
    "print(\"Sample captions:\")\n",
    "for c in image_to_captions[sample_img_id][:2]:\n",
    "    print(\"-\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fd78446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA GENERATOR\n",
    "# ============================================================================\n",
    "def caption_to_sequence(caption, word_to_idx):\n",
    "    return [word_to_idx[word] for word in caption.split() if word in word_to_idx]\n",
    "\n",
    "def data_generator(image_to_captions, image_features, word_to_idx,\n",
    "                   max_length, vocab_size, batch_size=32):\n",
    "\n",
    "    X_img, X_seq, y = [], [], []\n",
    "    \n",
    "    while True:\n",
    "        for img_id, captions in image_to_captions.items():\n",
    "            \n",
    "            feature = image_features[img_id]\n",
    "            \n",
    "            for caption in captions:\n",
    "                seq = caption_to_sequence(caption, word_to_idx)\n",
    "\n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq = seq[:i]\n",
    "                    out_word = seq[i]\n",
    "\n",
    "                    in_seq = pad_sequences(\n",
    "                        [in_seq],\n",
    "                        maxlen=max_length,\n",
    "                        padding='post'\n",
    "                    )[0]\n",
    "\n",
    "                    out_word = to_categorical(\n",
    "                        out_word,\n",
    "                        num_classes=vocab_size\n",
    "                    )\n",
    "\n",
    "                    X_img.append(feature)\n",
    "                    X_seq.append(in_seq)\n",
    "                    y.append(out_word)\n",
    "\n",
    "                    if len(X_img) == batch_size:\n",
    "                        yield [np.array(X_img), np.array(X_seq)], np.array(y)\n",
    "                        X_img, X_seq, y = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc745902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image input shape: (2, 2048)\n",
      "Seq input shape: (2, 32)\n",
      "Target shape: (2, 2567)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SANITY CHECK DATA GENERATOR\n",
    "# ============================================================================\n",
    "gen = data_generator(\n",
    "    image_to_captions=image_to_captions,\n",
    "    image_features=image_features,\n",
    "    word_to_idx=word_to_idx,\n",
    "    max_length=max_length,\n",
    "    vocab_size=vocab_size,\n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "(X_img, X_seq), y = next(gen)\n",
    "\n",
    "print(\"Image input shape:\", X_img.shape)   # (2, 2048)\n",
    "print(\"Seq input shape:\", X_seq.shape)     # (2, max_length)\n",
    "print(\"Target shape:\", y.shape)            # (2, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aae9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\Kuliah\\Semester 5\\Kecerdasan Buatan Lanjut\\Image-Captioning-Project\\.venv\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From f:\\Kuliah\\Semester 5\\Kecerdasan Buatan Lanjut\\Image-Captioning-Project\\.venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " seq_input (InputLayer)      [(None, 32)]                 0         []                            \n",
      "                                                                                                  \n",
      " image_input (InputLayer)    [(None, 2048)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 32, 256)              657152    ['seq_input[0][0]']           \n",
      "                                                                                                  \n",
      " image_dense (Dense)         (None, 256)                  524544    ['image_input[0][0]']         \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 (None, 256)                  525312    ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 256)                  0         ['image_dense[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 256)                  0         ['lstm[0][0]']                \n",
      "                                                                                                  \n",
      " merge (Add)                 (None, 256)                  0         ['dropout[0][0]',             \n",
      "                                                                     'dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 256)                  65792     ['merge[0][0]']               \n",
      "                                                                                                  \n",
      " output (Dense)              (None, 2567)                 659719    ['dense[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2432519 (9.28 MB)\n",
      "Trainable params: 2432519 (9.28 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BUILD MODEL\n",
    "# ============================================================================\n",
    "# Image feature input\n",
    "image_input = Input(shape=(2048,), name=\"image_input\")\n",
    "\n",
    "image_dense = Dense(256, activation=\"relu\", name=\"image_dense\")(image_input)\n",
    "image_dense = Dropout(0.5)(image_dense)\n",
    "\n",
    "# Caption sequence input\n",
    "seq_input = Input(shape=(max_length,), name=\"seq_input\")\n",
    "\n",
    "seq_embed = Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=256,\n",
    "    mask_zero=True,\n",
    "    name=\"embedding\"\n",
    ")(seq_input)\n",
    "\n",
    "seq_lstm = LSTM(256, name=\"lstm\")(seq_embed)\n",
    "seq_lstm = Dropout(0.5)(seq_lstm)\n",
    "\n",
    "# Merge Image & Text\n",
    "decoder = Add(name=\"merge\")([image_dense, seq_lstm])\n",
    "\n",
    "decoder = Dense(256, activation=\"relu\")(decoder)\n",
    "output = Dense(vocab_size, activation=\"softmax\", name=\"output\")(decoder)\n",
    "\n",
    "# Build & Compile Model\n",
    "model = Model(\n",
    "    inputs=[image_input, seq_input],\n",
    "    outputs=output\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\"\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b9a4530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 6401\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAINING CONFIG\n",
    "# ============================================================================\n",
    "# Training Config\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Steps per Epoch\n",
    "def count_steps(image_to_captions, batch_size):\n",
    "    total_pairs = 0\n",
    "    for captions in image_to_captions.values():\n",
    "        for c in captions:\n",
    "            total_pairs += len(c.split()) - 1\n",
    "    return total_pairs // batch_size\n",
    "\n",
    "steps_per_epoch = count_steps(image_to_captions, BATCH_SIZE)\n",
    "print(\"Steps per epoch:\", steps_per_epoch)\n",
    "\n",
    "# Data Generator\n",
    "train_generator = data_generator(\n",
    "    image_to_captions=image_to_captions,\n",
    "    image_features=image_features,\n",
    "    word_to_idx=word_to_idx,\n",
    "    max_length=max_length,\n",
    "    vocab_size=vocab_size,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Callbacks (Checkpoint & Early Stopping)\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=\"../checkpoints/caption_model_best.h5\",\n",
    "    monitor=\"loss\",\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"loss\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94af6b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 3.4205\n",
      "Epoch 1: loss improved from inf to 3.42048, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 531s 83ms/step - loss: 3.4205\n",
      "Epoch 2/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 3.3106\n",
      "Epoch 2: loss improved from 3.42048 to 3.31061, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 531s 83ms/step - loss: 3.3106\n",
      "Epoch 3/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 3.2430\n",
      "Epoch 3: loss improved from 3.31061 to 3.24304, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 531s 83ms/step - loss: 3.2430\n",
      "Epoch 4/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 3.1907\n",
      "Epoch 4: loss improved from 3.24304 to 3.19070, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 525s 82ms/step - loss: 3.1907\n",
      "Epoch 5/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 3.1539\n",
      "Epoch 5: loss improved from 3.19070 to 3.15390, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 528s 83ms/step - loss: 3.1539\n",
      "Epoch 6/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 3.1230\n",
      "Epoch 6: loss improved from 3.15390 to 3.12296, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 546s 85ms/step - loss: 3.1230\n",
      "Epoch 7/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 3.1001\n",
      "Epoch 7: loss improved from 3.12296 to 3.10014, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 543s 85ms/step - loss: 3.1001\n",
      "Epoch 8/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 3.0808\n",
      "Epoch 8: loss improved from 3.10014 to 3.08079, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 537s 84ms/step - loss: 3.0808\n",
      "Epoch 9/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 3.0623\n",
      "Epoch 9: loss improved from 3.08079 to 3.06235, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 539s 84ms/step - loss: 3.0623\n",
      "Epoch 10/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 3.0433\n",
      "Epoch 10: loss improved from 3.06235 to 3.04325, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 540s 84ms/step - loss: 3.0433\n",
      "Epoch 11/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 3.0296\n",
      "Epoch 11: loss improved from 3.04325 to 3.02957, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 536s 84ms/step - loss: 3.0296\n",
      "Epoch 12/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 3.0186\n",
      "Epoch 12: loss improved from 3.02957 to 3.01858, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 536s 84ms/step - loss: 3.0186\n",
      "Epoch 13/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 3.0076\n",
      "Epoch 13: loss improved from 3.01858 to 3.00762, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 545s 85ms/step - loss: 3.0076\n",
      "Epoch 14/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 2.9974\n",
      "Epoch 14: loss improved from 3.00762 to 2.99737, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 547s 85ms/step - loss: 2.9974\n",
      "Epoch 15/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 2.9893\n",
      "Epoch 15: loss improved from 2.99737 to 2.98935, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 541s 85ms/step - loss: 2.9893\n",
      "Epoch 16/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 2.9782\n",
      "Epoch 16: loss improved from 2.98935 to 2.97816, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 541s 84ms/step - loss: 2.9782\n",
      "Epoch 17/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 2.9744\n",
      "Epoch 17: loss improved from 2.97816 to 2.97437, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 541s 84ms/step - loss: 2.9744\n",
      "Epoch 18/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 2.9678\n",
      "Epoch 18: loss improved from 2.97437 to 2.96780, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 543s 85ms/step - loss: 2.9678\n",
      "Epoch 19/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 2.9603\n",
      "Epoch 19: loss improved from 2.96780 to 2.96026, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 540s 84ms/step - loss: 2.9603\n",
      "Epoch 20/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 2.9516\n",
      "Epoch 20: loss improved from 2.96026 to 2.95165, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 545s 85ms/step - loss: 2.9516\n",
      "Epoch 21/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 2.9483\n",
      "Epoch 21: loss improved from 2.95165 to 2.94833, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 544s 85ms/step - loss: 2.9483\n",
      "Epoch 22/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 2.9448\n",
      "Epoch 22: loss improved from 2.94833 to 2.94479, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 537s 84ms/step - loss: 2.9448\n",
      "Epoch 23/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 2.9434\n",
      "Epoch 23: loss improved from 2.94479 to 2.94341, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 536s 84ms/step - loss: 2.9434\n",
      "Epoch 24/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 2.9371\n",
      "Epoch 24: loss improved from 2.94341 to 2.93710, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 535s 84ms/step - loss: 2.9371\n",
      "Epoch 25/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 2.9330\n",
      "Epoch 25: loss improved from 2.93710 to 2.93299, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 536s 84ms/step - loss: 2.9330\n",
      "Epoch 26/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 2.9301\n",
      "Epoch 26: loss improved from 2.93299 to 2.93011, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 542s 85ms/step - loss: 2.9301\n",
      "Epoch 27/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 2.9305\n",
      "Epoch 27: loss did not improve from 2.93011\n",
      "6401/6401 [==============================] - 537s 84ms/step - loss: 2.9305\n",
      "Epoch 28/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 2.9226\n",
      "Epoch 28: loss improved from 2.93011 to 2.92262, saving model to ../checkpoints\\caption_model_best.h5\n",
      "6401/6401 [==============================] - 534s 83ms/step - loss: 2.9226\n",
      "Epoch 29/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 2.9245\n",
      "Epoch 29: loss did not improve from 2.92262\n",
      "6401/6401 [==============================] - 535s 84ms/step - loss: 2.9245\n",
      "Epoch 30/30\n",
      "6401/6401 [==============================] - ETA: 0s - loss: 2.9240\n",
      "Epoch 30: loss did not improve from 2.92262\n",
      "6401/6401 [==============================] - 536s 84ms/step - loss: 2.9240\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAINING MODEL\n",
    "# ============================================================================\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    callbacks=[checkpoint, early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
